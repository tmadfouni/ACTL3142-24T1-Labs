custom_cv = function(prune_cp = 0) {
mse = c()
folds = createFolds(train_data$Sales, k = 5)
for (validation_idx in folds) {
train = train_data[-validation_idx, ]
validation = train_data[validation_idx,]
model = rpart(Sales ~ ., train)
pruned = prune.rpart(model, prune_cp)
preds = predict(pruned, validation)
mse = c(mse, mean((preds - validation$Sales)^2))
}
return(mean(mse))
}
custom_cv(0)
custom_cv(0.1)
custom_cv(0.3)
custom_cv(0.7)
custom_cv(1)
library(glmnet)
help(cv.glmnet)
custom_cv = Vectorize(custom_cv)
plot(seq(0, 0.5, by = 0.05), custom_cv(seq(0, 0.5, by = 0.05)))
custom_cv = Vectorize(custom_cv)
plot(seq(0, 0.5, by = 0.05), custom_cv(seq(0, 0.5, by = 0.05)), type = "l")
library(randomForest)
bagging = randomForest(Sales ~ ., train_data, mtry = ncol(train_data) - 1, importance = T)
importance(bagging)
help(importance)
pred = predict(bagging, test_data)
mean((test_data$Sales - pred)^2)
mses = c()
for (i in 1:(ncol(train_data) - 1)) {
rf = randomForest(Sales ~ ., train_data, mtry = i)
pred = predict(rf, test_data)
mses = c(mses, mean((test_data$Sales - pred)^2))
}
print(mse)
print(mses)
plot(1:(ncol(train_data) - 1), mses)
plot(1:(ncol(train_data) - 1), mses, type = "l")
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = log(dataset$Salary, 10)
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-3, 2, by = 0.05)
for (lambda in lambdas) {
fit = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(fit, n.trees=1000)
test_pred = predict(fit, test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = log(dataset$Salary, 10)
train_data = dataset[1:200,]
test_data = dataset[201:nrow(dataset),]
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = log(dataset$Salary, 10)
train_data = dataset[1:200,]
test_data = dataset[201:nrow(dataset),]
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-3, 2, by = 0.05)
for (lambda in lambdas) {
fit = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(fit, n.trees=1000)
test_pred = predict(fit, test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
plot(lambdas, test_mses, type = "l")
lambdas
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(caret)
dataset = ISLR2::Carseats
set.seed(1)
createDataPartition(dataset$Sales, p = 0.8)
model = rpart(Sales ~ ., train_data)
library(rpart)
library(rpart.plot)
model = rpart(Sales ~ ., train_data)
set.seed(1)
train_idx = createDataPartition(dataset$Sales, p = 0.8)
train_data = dataset[train_idx,]
test_data = dataset[-train_idx,]
library(caret)
dataset = ISLR2::Carseats
set.seed(1)
train_idx = createDataPartition(dataset$Sales, p = 0.8)
train_data = dataset[train_idx,]
train_idx = createDataPartition(dataset$Sales, p = 0.8)$Resample1
train_data = dataset[train_idx,]
test_data = dataset[-train_idx,]
dataset = ISLR2::Carseats
set.seed(1)
train_idx = createDataPartition(dataset$Sales, p = 0.8)$Resample1
train_data = dataset[train_idx,]
test_data = dataset[-train_idx,]
library(rpart)
library(rpart.plot)
model = rpart(Sales ~ ., train_data)
plot(model)
text(model, pretty = 0)
plot(model)
text(model, pretty = 0)
plot(model)
text(model, pretty = 0)
rpart.plot(model)
rpart.plot(model)
pred = predict(model, newdata = test_data)
mean((pred - test_data$Sales)^2)
createFolds(train_data$Sales, k = 5)
help(prune.rpart)
custom_cv = function(prune_cp) {
mse = c()
folds = createFolds(train_data$Sales, k = 5)
for (validation_idx in folds) {
train = train_data[-validation_idx,]
validation = train_data[validation_idx,]
model = rpart(Sales ~ ., train)
pruned = prune(model, prune_cp)
pred = predict(pruned, validation)
mse = c(mse, mean((pred - validation$Sales)^2))
}
return(mean(mse))
}
custom_cv(0)
custom_cv(0.1)
custom_cv(0.3)
custom_cv(1)
cps = 10^seq(-2, 1, by = 0.05)
custom_cv = Vectorize(custom_cv)
plot(cps, custom_cv(cps))
custom_cv(cps)
plot(cps, custom_cv(cps), type = "l")
smth = custom_cv(cps)
smth
plot(cps, smth, type = "l")
smth
help(randomForest)
library(randomForest)
help(randomForest)
bagging = randomForest(Sales ~ ., train_data, mtry = ncol(train_data) - 1, importance = T)
bagging = randomForest(Sales ~ ., train_data, mtry = ncol(train_data) - 1, importance = T)
importance(bagging)
pred= predict(bagging, test_data)
mean((test_data$Sales - pred)^2)
mses = c()
for (i in 1:(ncol(train_data) - 1)) {
model = randomForest(Sales ~ ., train_data, mtry = i)
pred = predict(model, test_data)
mses = c(mses, mean((pred - test_data$Sales)^2))
}
for (i in 1:(ncol(train_data) - 1)) {
model = randomForest(Sales ~ ., train_data, mtry = i)
pred = predict(model, test_data)
mses = c(mses, mean((pred - test_data$Sales)^2))
}
plot(1:(ncol(train_data) - 1), mses, type = "l")
mses
ncol(train_data)
mses = c()
for (i in 1:(ncol(train_data) - 1)) {
model = randomForest(Sales ~ ., train_data, mtry = i)
pred = predict(model, test_data)
mses = c(mses, mean((pred - test_data$Sales)^2))
print(mses)
}
plot(1:(ncol(train_data) - 1), mses, type = "l")
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = dataset$Salary
View(dataset)
dataset$Salary = log(dataset$Salary, 10)
colSums(is.na(dataset))
library(gbm)
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = log(dataset$Salary, 10)
train_data = dataset[1:200,]
test_data = dataset[201:nrow(dataset),]
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, 0.2, by = 0.05)
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - train_pred)^2))
}
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, 0.2, by = 0.05)
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
plot(lambdas, train_mses, type = "l")
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, -0.2, by = 0.05)
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
plot(lambdas, train_mses, type = "l")
lambdas
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, -0.2, by = 0.05)
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
plot(lambdas, train_mses, type = "l")
lambdas
train_mses
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, -0.2, by = 0.05)
for (lambda in lambdas) {
model = gbm(Salary ~ ., distribution = "gaussian", data = train_data, n.trees = 1000, shrinkage = lambda)
train_pred = predict(model, n.trees = 1000)
test_pred = predict(model, newdata = test_data, n.trees = 1000)
train_mses = c(train_mses, mean((train_data$Salary - train_pred)^2))
test_mses = c(test_mses, mean((test_data$Salary - test_pred)^2))
}
plot(lambdas, train_mses, type = "l")
plot(lambdas, test_mses, type = "l")
plot(lambdas, train_mses, type = "l")
help("randomForest")
model = gbm(Salary~., data = train_data, shrinkage = 0.1, n.trees = 1000)
summary(model)
help(randomForest)
data = ISLR2::Carseats
str(data)
model = lm(Sales ~ Price, data = data)
plot(model)
par(mfrow= c(2,2))
plot(model)
library(cv.glmnet)
library(glmnet)
help(cv.glmnet)
knitr::opts_chunk$set(echo = TRUE)
dataset = ISRL2::Carseats
library(caret)
help("createDataPartition")
library(rpart)
library(rpart.plot)
v
model = rpart(Sales ~ ., traindata)
train_idx = createDataPartition(dataset$Sales, p=0.8)
traindata = dataset[train_idx, ]
testdata = dataset[-train_idx,]
dataset = ISLR2::Carseats
library(caret)
train_idx = createDataPartition(dataset$Sales, p=0.8)
traindata = dataset[train_idx, ]
testdata = dataset[-train_idx,]
createDataPartition(dataset$Sales, p=0.8)
train_idx = createDataPartition(dataset$Sales, p=0.8)$Resample1
traindata = dataset[train_idx, ]
testdata = dataset[-train_idx,]
library(rpart)
library(rpart.plot)
model = rpart(Sales ~ ., traindata)
summary(model)
plot(model)
text(model, pretty = 0)
createDataPartition(dataset$Sales, p=0.8)
plot(model)
text(model, pretty = 0)
rpart.plot(model)
plot(model)
text(model, pretty = 0)
rpart.plot(model)
pred = predict(model, newdata = testdata)
mean((testdata$Sales - pred)^2)
v
createFolds(traindata$Sales, k = 10)
custom_cv = function(prunecp) {
mse = c()
folds = createFolds(traindata$Sales, k = 10)
for (validationidx in folds) {
train = traindata[-validationidx,]
validation = traindata[validationidx,]
model = rpart(Sales ~ ., train)
pruned = prune.rpart(model, prunecp)
preds = predict(pruned, validation)
mse = c(mse, mean((preds - validation$Sales)^2))
}
return(mean(mses))
}
custom_cv(0)
custom_cv = function(prunecp) {
mse = c()
folds = createFolds(traindata$Sales, k = 10)
for (validationidx in folds) {
train = traindata[-validationidx,]
validation = traindata[validationidx,]
model = rpart(Sales ~ ., train)
pruned = prune.rpart(model, prunecp)
preds = predict(pruned, validation)
mse = c(mse, mean((preds - validation$Sales)^2))
}
return(mean(mse))
}
custom_cv(0)
custom_cv(0.001)
custom_cv(0.01)
cps = 10^seq(-2, 1, by = 0.05)
custom_cv = Vectorize(custom_cv)
plot(cps, custom_cvs(cps), type = "l"
plot(cps, custom_cvs(cps), type = "l")
plot(cps, custom_cv(cps), type = "l")
cps = 10^seq(-2, 0, by = 0.05)
custom_cv = Vectorize(custom_cv)
plot(cps, custom_cv(cps), type = "l")
help(randomForest)
library(randomForest)
help(randomForest)
bagging = randomForest(Sales ~ ., traindata, mtry = ncol(traindata) - 1, importance = T)
importance(bagging)
pred= predict(bagging, testdata)
mean((testdata$Sales - pred)^2)
mses = c()
for (i in 1:(ncol(traindata) - 1)) {
rf = randomForest(Sales ~ ., traindata, mtry = i)
pred = predict(rf, testdata)
mses = c(mses, mean((testdata$Sales - pred)^2))
}
for (i in 1:(ncol(traindata) - 1)) {
rf = randomForest(Sales ~ ., traindata, mtry = i)
pred = predict(rf, testdata)
mses = c(mses, mean((testdata$Sales - pred)^2))
}
mses = c()
for (i in 1:(ncol(traindata) - 1)) {
rf = randomForest(Sales ~ ., traindata, mtry = i)
pred = predict(rf, testdata)
mses = c(mses, mean((testdata$Sales - pred)^2))
}
plot(1:(ncol(traindata)-1), mses, type = "l")
model = randomForest(Sales ~ ., traindata, mtry = 4, importance = T)
importance(model)
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
colSums(is.na(dataset))
library(gbm)
help(gbm)
dataset = ISLR2::Hitters
colSums(is.na(dataset))
dataset = na.omit(dataset)
dataset$Salary = log(dataset$Salary, 10)
traindata = dataset[1:200,]
testdata = dataset[201:nrow(dataset),]
library(gbm)
train_mses = c()
test_mses = c()
lambdas = 10^seq(-5, -0.2, by = 0.05)
for (lambda in lambdas) {
fit = gbm(Salary ~ ., distribution = "gaussian", data = traindata, n.trees = 1000, shrinkage = lambda)
trainpred = predict(fit, n.trees=1000)
testpred= predict(fit, testdata, n.trees = 1000)
train_mses = c(train_mses, mean((traindata$Salary - trainpred)^2))
test_mses = c(test_mses, mean((testdata$Salary - testpred)^2))
}
plot(lambdas, train_mses, type = "l")
plot(lambdas, test_mses, type = "l")
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/tmadf/OneDrive - UNSW/University Tutoring/ACTL3142/24T2 Tutorial Git/week10/")
train = read.csv("mnist_small_train.csv")
train = read.csv("mnist_small_train.csv")
test = read.csv("mnist_test.csv")
library(randomForest)
start = Sys.time()
rf1 = randomForest(label ~ ., train, ntree=20, maxnodes = 100, do.trace=T)
end = Sys.time()
print(end - start)
pred = predict(rf1, test)
print(mean(pred == test$label))
pred
train$label = as.factor(train$label)
test$label = as.factor(test$label)
library(randomForest)
View(train)
colnames(train)
start = Sys.time()
rf1 = randomForest(label ~ ., train, ntree=20, maxnodes = 100, do.trace=T)
end = Sys.time()
print(end - start)
pred = predict(rf1, test)
print(mean(pred == test$label))
help(preProcess)
library(caret)
help("preProcess")
pca_X_data = preProcess(train[, -(ncol(train))], method = "pca", thresh = 0.95)
help(train.knnn)
help(train.kknn)
knitr::opts_chunk$set(echo = TRUE)
dataset = USArrests
View(dataset)
help("USArrests")
help(hclust)
help(dist)
h.model = hclust(dist(dataset))
plot(h.model)
cutree(h.model, k = 3)
help(scale)
scaled.dataset = scale(dataset, center = F, scale = T)
head(scaled.dataset)
head(dataset)
scaled.h.model = hclust(dist(scaled.dataset))
plot(scaled.h.model)
scaled.h.model = hclust(dist(scaled.dataset), method = "single")
plot(scaled.h.model)
setwd("C:/Users/tmadf/OneDrive - UNSW/University Tutoring/ACTL3142/24T2 Tutorial Git/week10/")
setwd("C:/Users/tmadf/OneDrive - UNSW/University Tutoring/ACTL3142/24T2 Tutorial Git/week10/")
train = read.csv("mnist_small_train.csv")
train = read.csv("mnist_small_train.csv")
test = read.csv("mnist_test.csv")
train = read.csv("mnist_small_train.csv")
train = read.csv("mnist_small_train.csv")
test = read.csv("mnist_test.csv")
View(train)
colnames(train)
which(c(3,4,6,9) == 4)
nonzero_var_cols = which(apply(X_train, 2, var) != 0)
zero_var_cols = which(apply(X_train, 2, var) == 0)
X_train = train[, -ncol(train)]
y_train = as.factor(train[, "label"])
X_test = test[, -ncol(test)]
y_test = as.factor(test[, "label"])
nonzero_var_cols = which(apply(X_train, 2, var) != 0)
nonzero_var_cols = which(apply(X_train, 2, var) != 0)
zero_var_cols = which(apply(X_train, 2, var) == 0)
v
start = Sys.time()
rf= randomForest(X_train, y_train, ntree = 20, maxnodes = 50, do.trace = T)
library(randomForest)
start = Sys.time()
rf= randomForest(X_train, y_train, ntree = 20, maxnodes = 50, do.trace = T)
print(Sys.time() - start)
pred = predict(rf, X_test)
print(mean(pred == y_test))
library(caret)
pca = preProcess(X_train, method = "pca", thresh = 0.95)
X_train = X_train[, nonzero_var_cols]
X_test = X_test[, nonzero_var_cols]
pca = preProcess(X_train, method = "pca", thresh = 0.95)
X_train_reduced = predict(pca, X_train)
X_train_reduced = predict(pca, X_train)
X_test_reduced = predict(pca, X_test)
train = read.csv("mnist_small_train.csv")
X_train = train[, -ncol(train)]
empties = matrix(data = NA, ncol = ncol(eigens), nrow = length(zero_var_cols), dimnames = list(names(zero_var_cols), colnames(eigens)))
eigens = pca$rotation
train = read.csv("mnist_small_train.csv")
X_train = train[, -ncol(train)]
empties = matrix(data = NA, ncol = ncol(eigens), nrow = length(zero_var_cols), dimnames = list(names(zero_var_cols), colnames(eigens)))
eigens = rbind(eigens, empties)
eigens = eigens[colnames(X_train),]
par(mfrow = c(3, 3), axes = F)
for (i in 1:9) {
m = matrix(data = eigens[,i], ncol = 28, nrow = 28)
image(1:ncol(m), 1:nrow(m), m, col = hcl.colors(60, palette = "grays",rev = T), axes = FALSE)
}
